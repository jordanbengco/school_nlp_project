{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation: Which aspect of the problem / task did your group choose to improve and reasons for your choice.\n",
    "\n",
    "Our group chose to compare how tf-idf and word2vec vector representation affect sentiment analysis/classification scoring accuracy on the Yelp dataset in different circumstances. We wanted to figure out which of the two models scored better on the Yelp dataset and if the scores changed depending on what review scores you include/exclude. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach: Describe the algorithms and machine learning models used in your project. Use a clear mathematical style to explain your model(s).\n",
    "\n",
    "The two models we used to score sentiment analysis accuracy were tf-idf and word2vec. The tf-idf representation of a term scores how important a word is to a document in a corpus.\n",
    "\n",
    "#### Tf-idf: \n",
    "\n",
    "TF(t) = (Number of times term t appears in the document) / (Total number of terms in the document) \n",
    "\n",
    "IDF(t) = log (Total number of documents) / (Number of documents with term t) \n",
    "\n",
    "Tf-idf (t) = TF(t) * IDF(t) \n",
    "        \n",
    "#### Word2Vec - CBOWs Model:\n",
    "\n",
    "The Word2vec representation took all words within a review and turned each word into its one-hot vector representation. It then combines all the vectors into one by taking the mean and matrix multiplying by the first trained word embedding matrix W. \n",
    "$$ h = W\\bar{x} $$ \n",
    "It then matrix multiply by the second word embedding W'. \n",
    "$$ u = W'u $$\n",
    "Then the final vector is calculated by taking the softmax of u.\n",
    "$$ y = Softmax(u) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data: Exactly which data files were used; also include here any external data that was not provided to you.\n",
    "\n",
    "The data we used was from the Yelp dataset challenge. The dataset had multiple json files that contains a range of information, most were unnecessary for our project. We extracted and used the stars and text from the review.json file. Out of the millions of reviews available, we only used the first 50000. https://www.yelp.com/dataset/challenge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code: If you used homework code, which homework code you used in your project. Provide exactly which code was used in your project not written by your group (e.g. use of an aligner from an open-source project).\n",
    "\n",
    "We did not use any homework code for this project.\n",
    "\n",
    "Our word2vec implementation was based on and adapted from claudiobellei.com/2018/01/07/backprop-word2vec-python/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimental Setup: Describe what kind of evaluation you are doing and which methods you are comparing against each other.\n",
    "\n",
    "We are using a support-vector machine model to score the accuracy of the two vector representation models in different circumstances to see how their accuracy changes. We tested the models on smaller training data sets of 200, 300, and 400 as well as a larger training set of 10,000. We also tested the two models accuracy on different subsets of the reviews. For example, one test we did was excluding 3 star (‘neutral’) reviews so that the sentiment was only positive or negative and comparing the results to each other as well as their previous scores when all 5 stars were included. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results : Include a detailed comparison of different methods.   \n",
    "\n",
    "| Model & Stars \\ Sample Size | 100 | 200 | 300 | 10,000 |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Tf-idf (all) | 56 | 52 | 58.67 | 58.56 |\n",
    "| Word2Vec (all) | 76 | 74 | 59 | 67 |\n",
    "| Tf-idf (1,2,4,5) | 60 | 60 | 61.33 | 65.27 |\n",
    "| Word2Vec (1,2,4,5) | 72 | 74 | 70.67 | 73.2 |\n",
    "| Tf-idf (1,5) | 72 | 92 | 90.67 | 95.07 | \n",
    "| Word2Vec (1,5) | 60 | 74 | 73.33 |76 |\n",
    "|Tf-idf (1,3,5) | 60 | 64 | 82.67 | 84.47 |\n",
    "| Word2Vec (1,3,5) | 56.0 | 58.0 | 65.33 | 63.28 |\n",
    "| Tf-idf (2,4) | 72 | 74 | 80 | 87.13 |\n",
    "| Word2Vec (2,4) | 88.0 | 74.0 | 73.3 | 73.16 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of the Results: Did you improve over the baseline. Why or why not?\n",
    "\n",
    "The tests we ran saw improvements and regressions in the model scores based on the inclusion and exclusion of certain stars as well as differing sample sizes. Larger training data sets improved the tf-idf model score significantly. This is most prominent in the extreme 1 and 5 star review subset where the score jumped from 72 to 92 with only 100 more reviews to train from. The Word2Vec CBOW model scores better on our initial test when considering all star ratings for all review sample sizes. Larger training data sets did not help the Word2Vec model increase its accuracy score. We believe the score does not significantly change due to the model being pre-trained on other data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future Work: What could be fixed in your approach. What you did not have time to finish, but you think would be a useful addition to your project.\n",
    "\n",
    "With more time or another group member we would have also liked to include the Skip-Gram model and GloVe embeddings and run the same circumstances and tests with those vector representations to see how their scores compared to the Word2vec and tf-idf models. We also would have liked to test the different circumstances with more sample sizes. Testing the data with sample sizes such as 1000, 2500 and 5000 could show us how the model accuracy changes as the sample size changes from small to medium sized. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
